{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporte y Entrenamiento del Modelo para el taller 1\n",
    "\n",
    "## Samuel Lopera Torres\n",
    "\n",
    "## Inteligencia Artificial\n",
    "\n",
    "## Escuela de Ciencias e Ingenieria\n",
    "\n",
    "### **Sobre los Hiperparametros**\n",
    "\n",
    "Tras varios experimentos, se definio la utilizacion de un learning rate de 0.1, 50 generaciones para el entrenamiento, y una probabilidad de dropout del 0.2, esto se debe que debido a la regularizacion del modelo, es algo mas resistente a cambios en la tasa de aprendizaje, y en los experimentos realizados utilizar una tasa menor resultaba en quedarse atascado en un minimo local para la funcion de costo, y utilizar tasas mayores y tiempos mayores de entrenamiento, resultaba en sobreentrenamiento (diferencia de 30 puntos porcentuales en el estimador de precision del modelo entre el dataset de entrenamiento y validacion).\n",
    "\n",
    "### **Estructura de la Red**\n",
    "\n",
    "La estructura de la red se inspira desde la estructura de redes profundas, buscando comprimir (en la medida de lo posible) el input inicial, con fin de facilitar el output de las clases para el modelo.\n",
    "\n",
    "En terminos generales, la estructura es la siguiente.\n",
    "\n",
    "Input (imagen aplanada en vector 150\\*150) -> Capa Lineal (150\\*150,11250) -> Batchnorm -> ReLu -> Dropout 0.2 ->\n",
    "Capa Lineal (11250,5625) -> Capa Lineal (5625,1125) -> Capa Lineal (1125, 256) -> Batchnorm -> ReLu -> Dropout 0.2 ->\n",
    "Capa Lineal (256,128) -> Capa Lineal (128,64) -> Batchnorm -> ReLu -> Dropout 0.2 ->\n",
    "Capa Lineal (64,6) -> Output\n",
    "\n",
    "Se toma en consideracion, que el modelo no va a presentar unas tasas de precision altas, debido a que la exclusiva utilizacion de capas lineales no es el acercamiento optimo a la clasificacion de imagenes,por lo que una precision mayor al 40% se considera un punto de parada decente para el modelo.\n",
    "\n",
    "### **Otras Consideraciones**\n",
    "\n",
    "- Para facilitar la lectura del codigo, las clases que permiten la implementacion de la red neuronal, estan en modulos en la carpeta raiz del proyecto, realizadas a mano durante la realizacion del minitorch workshop, presentado por el profesor como insumo para esta practica\n",
    "\n",
    "- Los datos del modelo estan en la carpeta data (importados de forma manual debido a no haber sido realizado en kaggle.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransforms\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/matplotlib/__init__.py:161\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcsetup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/matplotlib/rcsetup.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Don't let the original cycler collide with our validating cycler\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/matplotlib/_fontconfig_pattern.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lru_cache, partial\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyparsing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     Group, Optional, ParseException, Regex, StringEnd, Suppress, ZeroOrMore, oneOf)\n\u001b[1;32m     19\u001b[0m _family_punc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-:,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m _family_unescape \u001b[38;5;241m=\u001b[39m partial(re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m(?=[\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m])\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m _family_punc)\u001b[38;5;241m.\u001b[39msub, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pyparsing/__init__.py:132\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __diag__, __compat__\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresults\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pyparsing/core.py:389\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnull_debug_action\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"'Do-nothing' debug action, to suppress debugging output during parsing.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mParserElement\u001b[39;00m(ABC):\n\u001b[1;32m    390\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Abstract base level parser element class.\"\"\"\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     DEFAULT_WHITE_CHARS: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/abc.py:107\u001b[0m, in \u001b[0;36mABCMeta.__new__\u001b[0;34m(mcls, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__new__\u001b[39m(mcls, name, bases, namespace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(mcls, name, bases, namespace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 107\u001b[0m     \u001b[43m_abc_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split, Subset, Dataset\n",
    "from torch.utils.data import random_split\n",
    "from Net import *\n",
    "from Linear import *\n",
    "from CrossEntropyFromLogits import *\n",
    "from ReLU import *\n",
    "from Dropout import *\n",
    "from BatchNorm import *\n",
    "\n",
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((150, 150)),                # Resize images to 150x150\n",
    "    transforms.ToTensor(),                        # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])   # Normalize images to [0, 1]\n",
    "])\n",
    "\n",
    "# Load full train dataset\n",
    "train_path = 'data/seg_train/seg_train'\n",
    "full_trainset = datasets.ImageFolder(root=train_path, transform=transform)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Test data\n",
    "testpath = 'data/seg_test/seg_test'\n",
    "testset = datasets.ImageFolder(root=testpath, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Forcing the device to CPU (this line will override the previous check)\n",
    "print(device)\n",
    "\n",
    "# Define the number of input features and output classes\n",
    "n_features = 150*150\n",
    "n_classes = 6\n",
    "\n",
    "net = Net()\n",
    "\n",
    "net.add(Linear(n_features, 11250, device=device))\n",
    "#Complete Layer\n",
    "net.add(BatchNorm1D(n_features=11250, device= device))\n",
    "net.add(ReLU(device = device))\n",
    "net.add(Dropout(p = 0.2,device= device))\n",
    "#Complete Layer\n",
    "net.add(Linear(11250, 5625, device=device))\n",
    "net.add(Linear(5625, 1125, device=device))\n",
    "net.add(Linear(1125, 256, device=device))\n",
    "#Complete Layer\n",
    "net.add(BatchNorm1D(n_features=256, device= device))\n",
    "net.add(ReLU(device = device))\n",
    "net.add(Dropout(p = 0.2,device= device))\n",
    "#Complete Layer\n",
    "net.add(Linear(256,128,device=device))\n",
    "net.add(Linear(128, 64, device=device))\n",
    "#Complete Layer\n",
    "net.add(BatchNorm1D(n_features=64, device= device))\n",
    "net.add(ReLU(device = device))\n",
    "net.add(Dropout(p = 0.2,device= device))\n",
    "#Complete Layer\n",
    "net.add(Linear(64, n_classes, device=device))\n",
    "\n",
    "CELoss = CrossEntropyFromLogits()\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 0.1\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "batch_losses = []  # per-batch losses for the plot\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # -------- TRAIN --------\n",
    "    if hasattr(net, \"train\"): net.train()\n",
    "    running_loss, tot_correct, tot_samples = 0.0, 0, 0\n",
    "    total_batches = len(trainloader)\n",
    "\n",
    "    pbar = tqdm(trainloader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\")\n",
    "    for batch_idx, (images, labels) in enumerate(pbar, 1):\n",
    "        X = images.view(images.size(0), -1).to(device)\n",
    "        Y = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        Z = net.forward(X)\n",
    "        loss = CELoss.forward(Z, Y)\n",
    "\n",
    "        # Backward + update (manual autograd)\n",
    "        dZ = CELoss.backward(n_classes)\n",
    "        _ = net.backward(dZ)\n",
    "        net.update(learning_rate)\n",
    "\n",
    "        # Stats\n",
    "        running_loss += loss.item()\n",
    "        batch_losses.append(loss.detach().cpu().item())\n",
    "        _, predicted = torch.max(Z, 1)\n",
    "        tot_correct += (predicted == Y).sum().item()\n",
    "        tot_samples += Y.size(0)\n",
    "\n",
    "        if batch_idx % max(1, total_batches // 10) == 0:\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\",\n",
    "                             acc=f\"{tot_correct / max(1, tot_samples):.4f}\")\n",
    "\n",
    "    train_loss = running_loss / total_batches\n",
    "    train_acc = tot_correct / tot_samples\n",
    "\n",
    "    # -------- VALIDATION --------\n",
    "    if hasattr(net, \"eval\"): net.eval()\n",
    "    val_running_loss, val_correct, val_samples = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(valloader, desc=f\"Epoch {epoch}/{num_epochs} [Val]\"):\n",
    "            X = images.view(images.size(0), -1).to(device)\n",
    "            Y = labels.to(device)\n",
    "\n",
    "            Z = net.forward(X)\n",
    "            vloss = CELoss.forward(Z, Y)\n",
    "            val_running_loss += vloss.item()\n",
    "\n",
    "            _, predicted = torch.max(Z, 1)\n",
    "            val_correct += (predicted == Y).sum().item()\n",
    "            val_samples += Y.size(0)\n",
    "\n",
    "    val_loss = val_running_loss / len(valloader)\n",
    "    val_acc = val_correct / val_samples\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "# -------- OPTIONAL TEST --------\n",
    "if 'testloader' in globals() and testloader is not None:\n",
    "    if hasattr(net, \"eval\"): net.eval()\n",
    "    test_correct, test_samples, test_running_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(testloader, desc=\"[Test]\"):\n",
    "            X = images.view(images.size(0), -1).to(device)\n",
    "            Y = labels.to(device)\n",
    "            Z = net.forward(X)\n",
    "            loss = CELoss.forward(Z, Y)\n",
    "            test_running_loss += loss.item()\n",
    "            _, pred = torch.max(Z, 1)\n",
    "            test_correct += (pred == Y).sum().item()\n",
    "            test_samples += Y.size(0)\n",
    "    test_loss = test_running_loss / len(testloader)\n",
    "    test_acc = test_correct / test_samples\n",
    "    print(f\"[Test] Loss: {test_loss:.4f} | Acc: {test_acc:.4f}\")\n",
    "\n",
    "# -------- PLOTS --------\n",
    "plt.figure(); plt.plot(np.array(batch_losses))\n",
    "plt.xlabel('Batch'); plt.ylabel('Loss'); plt.title('Training Loss (per batch)'); plt.show()\n",
    "\n",
    "plt.figure(); plt.plot(history[\"train_loss\"], label='Train'); plt.plot(history[\"val_loss\"], label='Val')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.title('Loss per Epoch'); plt.show()\n",
    "\n",
    "plt.figure(); plt.plot(history[\"train_acc\"], label='Train'); plt.plot(history[\"val_acc\"], label='Val')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.title('Accuracy per Epoch'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en el output del entrenamiento, el modelo llega a una precision mayor al 40% en los sets de prueba y validacion, pese a que las graficas de la funcion de perdida y de la precision indican que utilizar una tasa de entrenamiento menor pudo haber sido beneficioso, se considera este rendimiento lo suficientemente bueno para el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV file 'predictions.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Root directory\n",
    "image_directory = 'data/seg_pred/seg_pred'\n",
    "\n",
    "# ✅ Your custom list of images\n",
    "image_files = ['6234.jpg', '22288.jpg', '20529.jpg', '21440.jpg', '5982.jpg', '7737.jpg', '2081.jpg',\n",
    "               '10054.jpg', '3919.jpg', '21838.jpg', '6399.jpg', '9960.jpg', '3757.jpg', '9131.jpg',\n",
    "               '9062.jpg', '4489.jpg', '3417.jpg', '6074.jpg', '7894.jpg', '10305.jpg', '4407.jpg',\n",
    "               '17720.jpg', '15979.jpg', '8983.jpg', '9697.jpg', '21590.jpg', '2628.jpg', '22604.jpg',\n",
    "               '16202.jpg', '3363.jpg', '13333.jpg', '14395.jpg', '6943.jpg', '3228.jpg', '12132.jpg',\n",
    "               '8075.jpg', '6365.jpg', '6459.jpg', '7690.jpg', '21966.jpg', '23566.jpg', '13087.jpg',\n",
    "               '4772.jpg', '21145.jpg', '6925.jpg', '10201.jpg', '15764.jpg', '7918.jpg', '16401.jpg',\n",
    "               '20789.jpg', '11617.jpg', '23258.jpg', '19651.jpg', '6800.jpg', '14376.jpg', '20321.jpg',\n",
    "               '12267.jpg', '18227.jpg', '4765.jpg', '22270.jpg', '21588.jpg', '6209.jpg', '5068.jpg',\n",
    "               '11529.jpg', '6229.jpg', '1749.jpg', '15360.jpg', '1995.jpg', '24068.jpg', '18048.jpg',\n",
    "               '12334.jpg', '20429.jpg', '3537.jpg', '21946.jpg', '2278.jpg', '11901.jpg', '12675.jpg',\n",
    "               '20308.jpg', '18399.jpg', '8929.jpg', '1971.jpg', '6248.jpg', '16993.jpg', '1383.jpg',\n",
    "               '9887.jpg', '20381.jpg', '20760.jpg', '11087.jpg', '6513.jpg', '18013.jpg', '22118.jpg',\n",
    "               '19382.jpg', '11848.jpg', '9486.jpg', '6493.jpg', '5734.jpg', '23082.jpg', '6797.jpg',\n",
    "               '20987.jpg', '18935.jpg', '6794.jpg', '13614.jpg', '11682.jpg', '19792.jpg', '22945.jpg',\n",
    "               '21493.jpg', '15578.jpg', '7516.jpg', '12205.jpg', '6952.jpg', '12941.jpg', '6059.jpg',\n",
    "               '24065.jpg', '23412.jpg', '22798.jpg', '13598.jpg', '19727.jpg', '5882.jpg', '20042.jpg',\n",
    "               '2532.jpg', '11849.jpg', '22108.jpg', '785.jpg', '2213.jpg', '18482.jpg', '18708.jpg',\n",
    "               '9336.jpg', '1777.jpg', '16360.jpg', '11895.jpg', '6924.jpg', '212.jpg', '19998.jpg',\n",
    "               '22445.jpg', '19165.jpg', '12838.jpg', '792.jpg', '2057.jpg', '21582.jpg', '17763.jpg',\n",
    "               '16406.jpg', '14737.jpg', '5158.jpg', '23924.jpg', '19596.jpg', '19079.jpg', '6917.jpg',\n",
    "               '10651.jpg', '10880.jpg', '17832.jpg', '2094.jpg', '16519.jpg', '2302.jpg', '21495.jpg',\n",
    "               '12421.jpg', '20578.jpg', '14919.jpg', '14476.jpg', '23881.jpg', '21809.jpg', '5458.jpg',\n",
    "               '2028.jpg', '14539.jpg', '4166.jpg', '1173.jpg', '3729.jpg', '6.jpg', '6941.jpg',\n",
    "               '8369.jpg', '13476.jpg', '7986.jpg', '20990.jpg', '1211.jpg', '8979.jpg', '8852.jpg',\n",
    "               '12464.jpg', '17836.jpg', '18626.jpg', '6715.jpg', '149.jpg', '22422.jpg', '23415.jpg',\n",
    "               '11422.jpg', '11678.jpg', '1644.jpg', '4494.jpg', '14730.jpg', '20839.jpg', '2640.jpg',\n",
    "               '436.jpg', '17103.jpg', '12141.jpg', '19654.jpg', '4439.jpg', '76.jpg', '21585.jpg',\n",
    "               '2844.jpg', '15027.jpg', '9257.jpg', '23424.jpg']\n",
    "\n",
    "# Custom dataset\n",
    "class SelectedFilesDataset(Dataset):\n",
    "    def __init__(self, root_dir, file_names, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_names = file_names\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.root_dir, self.file_names[idx])\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.file_names[idx]\n",
    "\n",
    "# Build dataset/loader\n",
    "comp_test_dataset = SelectedFilesDataset(image_directory, image_files, transform=transform)\n",
    "comp_test_loader = DataLoader(comp_test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "# Predict\n",
    "preds, ids = [], []\n",
    "with torch.no_grad():\n",
    "    for images, names in comp_test_loader:\n",
    "        X = images.view(images.shape[0], -1).to(device)\n",
    "        Z = net.forward(X)\n",
    "        _, predicted = torch.max(Z, 1)\n",
    "        preds.extend(predicted.detach().cpu().numpy())\n",
    "        ids.extend(names)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame({'id': ids, 'pred': preds})\n",
    "df.to_csv(\"predictions.csv\", index=False)\n",
    "print(\"✅ CSV file 'predictions.csv' created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
